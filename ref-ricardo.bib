@article{Camara2019,
abstract = {Understanding pedestrian behaviour and controlling interactions with pedestrians is of critical importance for autonomous vehicles, but remains a complex and challenging problem. This study infers pedestrian intent during possible road-crossing interactions, to assist autonomous vehicle decisions to yield or not yield when approaching them, and tests a simple heuristic model of intent on pedestrian-vehicle trajectory data for the first time. It relies on a heuristic approach based on the observed positions of the agents over time. The method can predict pedestrian crossing intent, crossing or stopping, with 96% accuracy by the time the pedestrian reaches the curbside, on the standard Daimler pedestrian dataset. This result is important in demarcating scenarios which have a clear winner and can be predicted easily with the simple heuristic, from those which may require more complex game-theoretic models to predict and control.},
author = {Camara, Fanta and Merat, Natasha and Fox, Charles W.},
doi = {10.1109/ITSC.2019.8917195},
file = {:D\:/Documents/USB/Bitacora/papers/paper 2.pdf:pdf},
isbn = {9781538670248},
journal = {2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
keywords = {Agent-Human Interactions,Autonomous Vehicles,Pedestrian Intention Crossing Estimation},
number = {723395},
pages = {3708--3713},
title = {{A heuristic model for pedestrian intention estimation}},
year = {2019}
}
@article{Fang2017,
abstract = {Avoiding vehicle-to-pedestrian crashes is a critical requirement for nowadays advanced driver assistant systems (ADAS) and future self-driving vehicles. Accordingly, detecting pedestrians from raw sensor data has a history of more than 15 years of research, with vision playing a central role. During the last years, deep learning has boosted the accuracy of image-based pedestrian detectors. However, detection is just the first step towards answering the core question, namely is the vehicle going to crash with a pedestrian provided preventive actions are not taken? Therefore, knowing as soon as possible if a detected pedestrian has the intention of crossing the road ahead of the vehicle is essential for performing safe and comfortable maneuvers that prevent a crash. However, compared to pedestrian detection, there is relatively little literature on detecting pedestrian intentions. This paper aims to contribute along this line by presenting a new vision-based approach which analyzes the pose of a pedestrian along several frames to determine if he or she is going to enter the road or not. We present experiments showing 750 ms of anticipation for pedestrians crossing the road, which at a typical urban driving speed of 50 km/h can provide 15 additional meters (compared to a pure pedestrian detector) for vehicle automatic reactions or to warn the driver. Moreover, in contrast with state-of-the-art methods, our approach is monocular, neither requiring stereo nor optical flow information.},
author = {Fang, Zhijie and V{\'{a}}zquez, David and L{\'{o}}pez, Antonio M.},
doi = {10.3390/s17102193},
file = {:D\:/Documents/USB/Bitacora/papers/potencial antecedente 1.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {ADAS,Pedestrian intention,Self-driving},
number = {10},
pages = {1--14},
pmid = {28946632},
title = {{On-board detection of pedestrian intentions}},
volume = {17},
year = {2017}
}
@article{Deng2009,
author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and Li, K. and Fei-Fei, L.},
journal = {IEEE conference on computer vision and pattern recognition},
pages = {248--255},
title = {{Imagenet: A large-scale hierarchical image database.}},
year = {2009}
}
@article{Kotseruba2021,
abstract = {Pedestrian action prediction has been a topic of active research in recent years resulting in many new algorithmic solutions. However, measuring the overall progress towards solving this problem is difficult due to the lack of publicly available benchmarks and common training and evaluation procedures. To this end, we introduce a benchmark based on two public datasets for pedestrian behavior understanding. Using the proposed evaluation procedures, we rank a number of baseline and state-of-the-art models and analyze their performance with respect to various properties of the data. Based on these findings we propose a new model for pedestrian crossing action prediction that uses attention mechanisms to effectively combine implicit and explicit features and demonstrate new state-of-the-art results. The code for models and evaluation is available at https://github.com/ykotseruba/PedestrianActionBenchmark.},
author = {Kotseruba, Iuliia and Rasouli, Amir and Tsotsos, John K.},
doi = {10.1109/WACV48630.2021.00130},
file = {:D\:/Documents/USB/Bitacora/papers/Benchmark_for_Evaluating_Pedestrian_Action_Prediction_WACV_2021_paper.pdf:pdf},
isbn = {9780738142661},
journal = {Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021},
pages = {1257--1267},
title = {{Benchmark for evaluating pedestrian action prediction}},
year = {2021}
}
@article{Schneider2013,
abstract = {In the context of intelligent vehicles, we perform a com-parative study on recursive Bayesian filters for pedestrian path pre-diction at short time horizons (< 2s). We consider Extended Kalman Filters (EKF) based on single dynamical models and Interacting Multi-ple Models (IMM) combining several such basic models (constant veloc-ity/acceleration/turn). These are applied to four typical pedestrian mo-tion types (crossing, stopping, bending in, starting). Position measure-ments are provided by an external state-of-the-art stereo vision-based pedestrian detector. We investigate the accuracy of position estimation and path prediction, and the benefit of the IMMs vs. the simpler single dynamical models. Special care is given to the proper sensor modeling and parameter optimization. The dataset and evaluation framework are made public to facilitate benchmarking.},
author = {Schneider, N ; and Gavrila, D M and Schneider, N},
doi = {10.1007/978-3-642-40602-7},
file = {:D\:/Documents/USB/Bitacora/papers/primera base de datos.pdf:pdf},
isbn = {9783642406027},
journal = {Lecture Notes in Computer Science},
pages = {174--183},
title = {{UvA-DARE (Digital Academic Repository) Pedestrian Path Prediction with Recursive Bayesian Filters: A Comparative Study Pedestrian Path Prediction with Recursive Bayesian Filters: A Comparative Study}},
url = {https://pure.uva.nl/ws/files/1696479/157078_gcpr13.pdf},
volume = {8142},
year = {2013}
}
@article{Rasouli2020,
abstract = {One of the major challenges for autonomous vehicles in urban environments is to understand and predict other road users' actions, in particular, pedestrians at the point of crossing. The common approach to solving this problem is to use the motion history of the agents to predict their future trajectories. However, pedestrians exhibit highly variable actions most of which cannot be understood without visual observation of the pedestrians themselves and their surroundings. To this end, we propose a solution for the problem of pedestrian action anticipation at the point of crossing. Our approach uses a novel stacked RNN architecture in which information collected from various sources, both scene dynamics and visual features, is gradually fused into the network at different levels of processing. We show, via extensive empirical evaluations, that the proposed algorithm achieves a higher prediction accuracy compared to alternative recurrent network architectures. We conduct experiments to investigate the impact of the length of observation, time to event and types of features on the performance of the proposed method. Finally, we demonstrate how different data fusion strategies impact prediction accuracy.},
archivePrefix = {arXiv},
arxivId = {2005.06582},
author = {Rasouli, Amir and Kotseruba, Iuliia and Tsotsos, John K.},
eprint = {2005.06582},
file = {:C\:/Users/ricar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsotsos, Rasouli, Kotseruba - 2020 - Pedestrian action anticipation using contextual feature fusion in stacked RNNs y 2020.pdf:pdf},
journal = {30th British Machine Vision Conference 2019, BMVC 2019},
pages = {1--13},
title = {{Pedestrian action anticipation using contextual feature fusion in stacked RNNs}},
year = {2020}
}
@article{Merri2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1259v2},
author = {Merri, Bart Van},
eprint = {arXiv:1409.1259v2},
file = {:D\:/Documents/USB/Bitacora/papers/GRUs.pdf:pdf},
title = {{On the Properties of Neural Machine Translation: Encoderâ€“Decoder Approaches}},
year = {2014}
}
@article{Lorenzo2021,
abstract = {Anticipating pedestrian crossing behavior in urban scenarios is a challenging task for autonomous vehicles. Early this year, a benchmark comprising JAAD and PIE datasets have been released. In the benchmark, several state-of-the-art methods have been ranked. However, most of the ranked temporal models rely on recurrent architectures. In our case, we propose, as far as we are concerned, the first self-attention alternative, based on transformer architecture, which has had enormous success in natural language processing (NLP) and recently in computer vision. Our architecture is composed of various branches which fuse video and kinematic data. The video branch is based on two possible architectures: RubiksNet and TimeSformer. The kinematic branch is based on different configurations of transformer encoder. Several experiments have been performed mainly focusing on pre-processing input data, highlighting problems with two kinematic data sources: pose keypoints and ego-vehicle speed. Our proposed model results are comparable to PCPA, the best performing model in the benchmark reaching an F1 Score of nearly 0.78 against 0.77. Furthermore, by using only bounding box coordinates and image data, our model surpasses PCPA by a larger margin (F1 = 0.75 vs. F1 = 0.72). Our model has proven to be a valid alternative to recurrent architectures, providing advantages such as parallelization and whole sequence processing, learning relationships between samples not possible with recurrent architectures.},
author = {Lorenzo, Javier and Alonso, Ignacio Parra and Izquierdo, Rub{\'{e}}n and Ballardini, Augusto Luis and Saz, {\'{A}}lvaro Hern{\'{a}}ndez and Llorca, David Fern{\'{a}}ndez and Sotelo, Miguel {\'{A}}ngel},
doi = {10.3390/s21175694},
file = {:D\:/Documents/USB/Bitacora/papers/capformer.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Action classification,Autonomous vehicles,Deep learning,Pedestrian,Prediction,Transformer},
number = {17},
pages = {1--22},
pmid = {34502584},
title = {{CAPformer: Pedestrian crossing action prediction using transformer}},
volume = {21},
year = {2021}
}
@article{Dafrallah2019,
abstract = {Thousands of people are dying every year due to road accidents; in fact 23% of world fatal accidents are pedestrians related, where 40% of them occur in Africa as reported by the World Health Organisation (WHO). Predicting the walking direction of a pedestrian could help to avoid an eventual accident. Existing studies can not handle pose and orientation transformations of the input object contrary to our proposed method. This paper describes a novel approach to determine the pedestrian orientation using Capsule Networks (CapsNet) based scheme. CapsNet are a new deep learning architecture that overcome some limitations of the existing studies, they are group of neurons invariant to rotation and affine transformations, which represent a specific interest to this work. Capsule Networks predicts the walking directions of pedestrians to prevent such mortal accidents, using four main walking directions (front, back, left and right).For this purpose, a new pedestrians dataset gathered from the most popular cities in Morocco is collected to be studied and used as a proof of the proposed approach. To enhance this proposed approach, we evaluated it using Daimler dataset and compared it to Convolutional Neural Networks (CNN) architectures.Experimental results reveal that the performance of the proposed approach reaches an accuracy of 97.60% on daimler dataset and 73.64% on our Moroccan collected dataset.},
author = {Dafrallah, Safaa and Amine, Aouatif and Mousset, Stephane and Bensrhair, Abdelaziz},
doi = {10.1109/ITSC.2019.8917019},
file = {:D\:/Documents/USB/Bitacora/papers/paper 4.pdf:pdf},
isbn = {9781538670248},
journal = {2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
pages = {3702--3707},
title = {{Will Capsule Networks overcome Convolutional Neural Networks on Pedestrian Walking Direction ?}},
year = {2019}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 Ã— 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16â€“19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:D\:/Documents/USB/Bitacora/papers/VGG16.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Aggarwal2014,
abstract = {This chapter will summarize the key resources available in the literature on data classification. While this book provides a basic understanding of the important aspects of data classification, these resources will provide the researcher more in-depth perspectives on each individual topic. Therefore, this chapter will summarize the key resources in this area. In general, the resources on data classification can be divided into the categories of (i) books, (ii) survey articles, and (iii) software.},
author = {Aggarwal, Charu C.},
doi = {10.1201/b17320},
file = {:D\:/Documents/USB/Bitacora/papers/transfer learning.pdf:pdf},
isbn = {9781466586758},
journal = {Data Classification: Algorithms and Applications},
pages = {657--665},
title = {{Educational and software resources for data classification}},
year = {2014}
}
@article{Cao2017,
abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a non-parametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efficiency.},
archivePrefix = {arXiv},
arxivId = {1611.08050},
author = {Cao, Zhe and Simon, Tomas and Wei, Shih En and Sheikh, Yaser},
doi = {10.1109/CVPR.2017.143},
eprint = {1611.08050},
file = {:D\:/Documents/USB/Bitacora/papers/openpose.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1302--1310},
title = {{Realtime multi-person 2D pose estimation using part affinity fields}},
volume = {2017-Janua},
year = {2017}
}
@article{Rasouli2020a,
abstract = {One of the major challenges for autonomous vehicles in urban environments is to understand and predict other road users' actions, in particular, pedestrians at the point of crossing. The common approach to solving this problem is to use the motion history of the agents to predict their future trajectories. However, pedestrians exhibit highly variable actions most of which cannot be understood without visual observation of the pedestrians themselves and their surroundings. To this end, we propose a solution for the problem of pedestrian action anticipation at the point of crossing. Our approach uses a novel stacked RNN architecture in which information collected from various sources, both scene dynamics and visual features, is gradually fused into the network at different levels of processing. We show, via extensive empirical evaluations, that the proposed algorithm achieves a higher prediction accuracy compared to alternative recurrent network architectures. We conduct experiments to investigate the impact of the length of observation, time to event and types of features on the performance of the proposed method. Finally, we demonstrate how different data fusion strategies impact prediction accuracy.},
archivePrefix = {arXiv},
arxivId = {2005.06582},
author = {Rasouli, Amir and Kotseruba, Iuliia and Tsotsos, John K.},
eprint = {2005.06582},
file = {:D\:/Documents/USB/Bitacora/papers/bmvc_19_poster.pdf:pdf},
journal = {30th British Machine Vision Conference 2019, BMVC 2019},
pages = {60},
title = {{Pedestrian action anticipation using contextual feature fusion in stacked RNNs}},
year = {2020}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Rosenblatt, F.},
journal = {Psychological review},
number = {6},
pages = {386--408},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}
@article{Morales-Alvarez2018,
abstract = {The work presented on this paper, aims to provide an automated tool to analyze the autonomous vehicle-pedestrian interaction. It detects pedestrians in the environment, by means of the most modern visual detection technology available in the literature, applied to an stereo camera system, and analyses if the driver looks at the vehicle. To achieve this task the algorithm does pose estimation, feature matching, and facial detection to acquire the position of the pedestrians and distinguish if they notice the vehicle. The algorithm calculates the 3D coordinates of a given pedestrian using the vehicle as a reference, tracking the movement of the pedestrian during all the process, thus providing meta-information of this interaction, which allows to process this information at higher levels: e.g. if the pedestrians feels conformable to cross, or if he performs any other maneuver moving away from the trajectory. The proposed algorithm was tested in campus scenarios where pedestrians and vehicle shared the environment, and the results proved the viability, providing a tool, useful for researchers which allows to process high amounts of information without the need of supervision.},
author = {Morales-{\'{A}}lvarez, Walter and G{\'{o}}mez-Silva, Mara Jos{\'{e}} and Fern{\'{a}}ndez-L{\'{o}}pez, Gerardo and Garca-Fern{\'{a}}ndez, Fernando and Olaverri-Monreal, Cristina},
doi = {10.1109/IVS.2018.8500425},
file = {:D\:/Documents/USB/Bitacora/papers/walter.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
keywords = {autonomous vehicles,behavior analysis,human-computer interaction,pedestrian identification},
number = {Iv},
pages = {1--6},
title = {{Automatic Analysis of Pedestrian's Body Language in the Interaction with Autonomous Vehicles}},
volume = {2018-June},
year = {2018}
}
@article{Czart2004,
abstract = {The Penson-Kolb-Hubbard model, i.e. the Hubbard model with the pair-hopping interaction J is studied. We focus on the properties of the superconducting state with the Cooper-pair center-of-mass momentum q = Q ($\eta$-phase). The transition into the $\eta$-phase, which is favorized by the repulsive J (J < 0) is found to occur only above some critical value |J c|, dependent on band filling, on-site interaction U and band structure, and the system never exhibits standard BCS-like features. This is in obvious contrast with the properties of the isotropic s-wave state, stabilized by the attractive J and attractive U, which exhibit at T = 0 a smooth crossover from the BCS-like limit to that of tightly bound pairs with increasing pairing strength.},
author = {Czart, W. R. and Robaszkiewicz, S.},
doi = {10.12693/APhysPolA.106.709},
file = {:D\:/Documents/USB/Bitacora/papers/pose de fulano.pdf:pdf},
issn = {05874246},
journal = {Acta Physica Polonica A},
number = {5},
pages = {709--713},
title = {{Superconducting properties of the $\eta$-pairing state in the Penson - Kolb - Hubbard model}},
volume = {106},
year = {2004}
}
@article{Rasouli2017,
abstract = {Designing autonomous vehicles suitable for urban environments remains an unresolved problem. One of the major dilemmas faced by autonomous cars is how to understand the intention of other road users and communicate with them. The existing datasets do not provide the necessary means for such higher level analysis of traffic scenes. With this in mind, we introduce a novel dataset which in addition to providing the bounding box information for pedestrian detection, also includes the behavioral and contextual annotations for the scenes. This allows combining visual and semantic information for better understanding of pedestrians' intentions in various traffic scenarios. We establish baseline approaches for analyzing the data and show that combining visual and contextual information can improve prediction of pedestrian intention at the point of crossing by at least 20%.},
author = {Rasouli, Amir and Kotseruba, Iuliia and Tsotsos, John K.},
doi = {10.1109/ICCVW.2017.33},
file = {:D\:/Documents/USB/Bitacora/papers/Rasouli_Are_They_Going_ICCV_2017_paper.pdf:pdf},
isbn = {9781538610343},
journal = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
pages = {206--213},
title = {{Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior}},
volume = {2018-Janua},
year = {2017}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and J{\"{u}}rgen, Schmidhuber},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Lorenzo2021a,
abstract = {Understanding pedestrian crossing behavior is an essential goal in intelligent vehicle development, leading to an improvement in their security and traffic flow. In this paper, we developed a method called IntFormer. It is based on transformer architecture and a novel convolutional video classification model called RubiksNet. Following the evaluation procedure in a recent benchmark, we show that our model reaches state-of-the-art results with good performance ($\approx 40$ seq. per second) and size ($8\times $smaller than the best performing model), making it suitable for real-time usage. We also explore each of the input features, finding that ego-vehicle speed is the most important variable, possibly due to the similarity in crossing cases in PIE dataset.},
archivePrefix = {arXiv},
arxivId = {2105.08647},
author = {Lorenzo, J. and Parra, I. and Sotelo, M. A.},
eprint = {2105.08647},
file = {:D\:/Documents/USB/Bitacora/papers/intformer.pdf:pdf},
title = {{IntFormer: Predicting pedestrian intention with the aid of the Transformer architecture}},
url = {http://arxiv.org/abs/2105.08647},
year = {2021}
}
@article{Zhan2019,
abstract = {In the field of autonomous driving, pedestrian detection and pedestrian behavior identification are important in ensuring pedestrian safety and reducing traffic accidents. Facing the complex traffic scene of the city, we propose a vision-based, high-accuracy and real-time pedestrian detection and pedestrian behavior recognition method for the on-vehicle environment. The method firstly uses the YOLOv3-TINY network to accurately and quickly identify pedestrians, and then uses the improved Deepsort algorithm to track the identified pedestrians. An improved Alexnet was then proposed to identify pedestrian behavior and train on data sets adapted to the study based on the mars dataset. After getting the pedestrian behavior, the behavior is corrected by multi-target tracking, and finally the real-time warning level information is given. Compared with similar algorithms, the experimental results show that the algorithm proposed in this paper improves the recognition accuracy and real-time performance, and the frame rate reaches 20frame/s.},
author = {Zhan, Huiqin and Liu, Yuan and Cui, Zhibin and Cheng, Hong},
doi = {10.1109/ITSC.2019.8917264},
file = {:D\:/Documents/USB/Bitacora/papers/paper 3.pdf:pdf},
isbn = {9781538670248},
journal = {2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
pages = {771--776},
title = {{Pedestrian Detection and Behavior Recognition Based on Vision}},
year = {2019}
}
@article{Yen1996,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Yen, Ti-Yen and Wolf, Wayne},
doi = {10.1007/978-1-4757-5388-2_2},
file = {:D\:/Documents/USB/Bitacora/papers/LSTM.pdf:pdf},
journal = {Hardware-Software Co-Synthesis of Distributed Embedded Systems},
number = {8},
pages = {13--39},
title = {{Previous Work}},
volume = {9},
year = {1996}
}
@article{Ludl2019,
abstract = {Recognizing human actions is a core challenge for autonomous systems as they directly share the same space with humans. Systems must be able to recognize and assess human actions in real-time. To train the corresponding data-driven algorithms, a significant amount of annotated training data is required. We demonstrate a pipeline to detect humans, estimate their pose, track them over time and recognize their actions in real-time with standard monocular camera sensors. For action recognition, we transform noisy human pose estimates in an image like format we call Encoded Human Pose Image (EHPI). This encoded information can further be classified using standard methods from the computer vision community. With this simple procedure, we achieve competitive state-of-the-art performance in pose-based action detection and can ensure real-time performance. In addition, we show a use case in the context of autonomous driving to demonstrate how such a system can be trained to recognize human actions using simulation data.},
archivePrefix = {arXiv},
arxivId = {1904.09140},
author = {Ludl, Dennis and Gulde, Thomas and Curio, Cristobal},
doi = {10.1109/ITSC.2019.8917128},
eprint = {1904.09140},
file = {:D\:/Documents/USB/Bitacora/papers/paper 5.pdf:pdf},
isbn = {9781538670248},
journal = {2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
pages = {581--588},
title = {{Simple yet efficient real-time pose-based action recognition}},
volume = {581},
year = {2019}
}
@article{Fan2020,
abstract = {Video action recognition is a complex task dependent on modeling spatial and temporal context. Standard approaches rely on 2D or 3D convolutions to process such context, resulting in expensive operations with millions of parameters. Recent efficient architectures leverage a channel-wise shift-based primitive as a replacement for temporal convolutions, but remain bottlenecked by spatial convolution operations to maintain strong accuracy and a fixed-shift scheme. Naively extending such developments to a 3D setting is a difficult, intractable goal. To this end, we introduce RubiksNet, a new efficient architecture for video action recognition which is based on a proposed learnable 3D spatiotemporal shift operation instead. We analyze the suitability of our new primitive for video action recognition and explore several novel variations of our approach to enable stronger representational flexibility while maintaining an efficient design. We benchmark our approach on several standard video recognition datasets, and observe that our method achieves comparable or better accuracy than prior work on efficient video action recognition at a fraction of the performance cost, with 2.9â€“5.9Ã— fewer parameters and 2.1â€“3.7Ã— fewer FLOPs. We also perform a series of controlled ablation studies to verify our significant boost in the efficiency-accuracy tradeoff curve is rooted in the core contributions of our RubiksNet architecture.},
author = {Fan, Linxi and Buch, Shyamal and Wang, Guanzhi and Cao, Ryan and Zhu, Yuke and Niebles, Juan Carlos and Fei-Fei, Li},
doi = {10.1007/978-3-030-58529-7_30},
file = {:D\:/Documents/USB/Bitacora/papers/rubisknet.pdf:pdf},
isbn = {9783030585280},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Budget-constrained,Efficient action recognition,Learnable shift,Spatiotemporal,Video understanding},
pages = {505--521},
title = {{RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition}},
volume = {12364 LNCS},
year = {2020}
}
@article{Bertasius2021,
abstract = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
archivePrefix = {arXiv},
arxivId = {2102.05095},
author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
eprint = {2102.05095},
file = {:D\:/Documents/USB/Bitacora/papers/timesformer.pdf:pdf},
title = {{Is Space-Time Attention All You Need for Video Understanding?}},
url = {http://arxiv.org/abs/2102.05095},
year = {2021}
}
@article{Lecun,
author = {Lecun, Yann and Kavukcuoglu, Koray},
file = {:D\:/Documents/USB/Bitacora/papers/CNN.pdf:pdf},
journal = {Proceedings of 2010 IEEE International Symposium on Circuits and Systems.},
pages = {253--256},
title = {{IEEE Xplore - Convolutional networks and applications in vision}},
year = {2010}
}
@article{wiki1,
title = {El perceptrÃ³n},
url = {https://es.wikipedia.org/wiki/Perceptr\%C3\%B3n} 
}

@article{wiki2,
title = {Red neuronal completamente conectada},
url = {https://www.ibm.com/docs/es/spss-modeler/SaaS?topic=networks-neural-model} 
}

@article{wiki3,
title = {Red neuronal convolucional},
url = {https://towardsdatascience.com/how-to-teach-a-computer-to-see-with-convolutional-neural-networks-96c120827cd1} 
}

@article{wiki4,
title = {EspecializaciÃ³n de Deep learning en coursera},
url = {https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning} 
}

@article{wiki5,
title = {Dot CSV explicaciÃ³n del codificador posicional},
url = {https://www.youtube.com/watch?v=xi94v_jl26U&t=609s} 
}

@article{wiki6,
title = {Pytorch Lightning},
url = {https://www.pytorchlightning.ai/} 
}

@article{wiki7,
title = {Repositorio de la Arquitectura SF-GRU},
url = {https://github.com/aras62/SF-GRU} 
}

@article{wiki8,
title = {Repositorio de la base de datos PIE},
url = {https://github.com/aras62/PIE} 
}
    
@article{wiki9,
title = {Repositorio de la base de datos JAAD},
url = {https://github.com/ykotseruba/JAAD} 
}

@article{wiki10,
title = {Torch Metrics, mÃ©trica AUC},
url = {https://torchmetrics.readthedocs.io/en/stable/references/modules.html#auc} 
}

@article{wiki11,
title = {Supervised learning and other branches of machine learning},
url = {https://blog.superannotate.com/supervised-learning-and-other-machine-learning-tasks/?utm_term=&utm_campaign=Annotation_Search_New_Segmented&utm_source=adwords&utm_medium=ppc&hsa_acc=7527629942&hsa_cam=14073003037&hsa_grp=132203422229&hsa_ad=577732942220&hsa_src=g&hsa_tgt=dsa-901208179420&hsa_kw=&hsa_mt=&hsa_net=adwords&hsa_ver=3&gclid=Cj0KCQjwz7uRBhDRARIsAFqjullkNTakGtf2AZ5g9xSyIa047BSohFT-kGG2qF_WObtkJFn7PqdZVckaApjgEALw_wcB}
}


@article{wiki12,
title = {Machine Learning, Backpropagation},
url = {https://en.wikipedia.org/wiki/Backpropagation}
}

@article{Niu2021,
   abstract = {Attention has arguably become one of the most important concepts in the deep learning field. It is inspired by the biological systems of humans that tend to focus on the distinctive parts when processing large amounts of information. With the development of deep neural networks, attention mechanism has been widely used in diverse application domains. This paper aims to give an overview of the state-of-the-art attention models proposed in recent years. Toward a better general understanding of attention mechanisms, we define a unified model that is suitable for most attention structures. Each step of the attention mechanism implemented in the model is described in detail. Furthermore, we classify existing attention models according to four criteria: the softness of attention, forms of input feature, input representation, and output representation. Besides, we summarize network architectures used in conjunction with the attention mechanism and describe some typical applications of attention mechanism. Finally, we discuss the interpretability that attention brings to deep learning and present its potential future trends.},
   author = {Zhaoyang Niu and Guoqiang Zhong and Hui Yu},
   doi = {10.1016/J.NEUCOM.2021.03.091},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Attention mechanism,Computer vision applications,Convolutional Neural Network (CNN),Deep learning,Encoder-decoder,Natural language processing applications,Recurrent Neural Network (RNN),Unified attention model},
   month = {9},
   pages = {48-62},
   publisher = {Elsevier},
   title = {A review on the attention mechanism of deep learning},
   volume = {452},
   year = {2021},
}


@article{Rasouli2019,
abstract = {Pedestrian behavior anticipation is a key challenge in the design of assistive and autonomous driving systems suitable for urban environments. An intelligent system should be able to understand the intentions or underlying motives of pedestrians and to predict their forthcoming actions. To date, only a few public datasets were proposed for the purpose of studying pedestrian behavior prediction in the context of intelligent driving. To this end, we propose a novel large-scale dataset designed for pedestrian intention estimation (PIE). We conducted a large-scale human experiment to establish human reference data for pedestrian intention in traffic scenes. We propose models for estimating pedestrian crossing intention and predicting their future trajectory. Our intention estimation model achieves 79% accuracy and our trajectory prediction algorithm outperforms state-of-the-art by 26% on the proposed dataset. We further show that combining pedestrian intention with observed motion improves trajectory prediction. The dataset and models are available at http://data.nvision2.eecs.yorku.ca/PIE-dataset/.},
author = {Rasouli, Amir and Kotseruba, Iuliia and Kunic, Toni and Tsotsos, John},
doi = {10.1109/ICCV.2019.00636},
file = {:D\:/Documents/USB/Bitacora/papers/PIE paper.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {6261--6270},
title = {{PIE: A large-scale dataset and models for pedestrian intention estimation and trajectory prediction}},
volume = {2019-Octob},
year = {2019}
}
@article{Vaswani2017,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz},
doi = {10.1109/2943.974352},
file = {:D\:/Documents/USB/Bitacora/papers/attention-is-all-you-need-Paper.pdf:pdf},
issn = {10772618},
journal = {Advances in neural information processing systems},
pages = {5598--6008},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Volz2016,
abstract = {In the context of future urban automated driving many important problems remain unsolved. A critical one is the analysis and prediction of pedestrian movements around urban roads. Especially the analysis of non-critical situations has not received much attention in the past. This paper focuses on analyzing and predicting movements of pedestrians approaching crosswalks, a very crucial pedestrian-vehicle interaction in urban scenarios. In our previous work, we analyzed the performance of a data-driven Support Vector Machine-based architecture, and the relevance of specific features to infer pedestrian crossing intentions. In this paper, we will use our previous results as baseline to compare against an architecture based on neural networks for time-series classification. In particular we analyze the effectiveness of dense and Long- Short-Term-Memory networks. Furthermore, we will be looking into enhancing our feature vectors by adding LiDAR based images to the classification process. Additionally the evaluation provides an estimate for the temporal prediction horizon. The approaches presented are validated with real world trajectories recorded in Germany. Our results show an average accuracy improvement of 1020% with respect to our previous Support Vector Machine-based approach.},
author = {V{\"{o}}lz, Benjamin and Behrendt, Karsten and Mielenz, Holger and Gilitschenski, Igor and Siegwart, Roland and Nieto, Juan},
doi = {10.1109/ITSC.2016.7795975},
file = {:D\:/Documents/USB/Bitacora/papers/potencial antecedente 2.pdf:pdf},
isbn = {9781509018895},
journal = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
pages = {2607--2612},
title = {{A data-driven approach for pedestrian intention estimation}},
year = {2016}
}
@article{Bighashdel2019,
abstract = {Behavior analysis of Vulnerable Road Users (VRU)s has become a crucial topic in the computer vision research area. In recent decades, numerous papers have extensively addressed the problem of VRU path prediction, which has a wide range of applications such as video surveillance and autonomous driving. The behavioral complexity of VRUs has forced researchers to employ various techniques in order to develop more comprehensive models that potentially respond better to VRUs movement patterns. This indeed has led to development of a large variety of models and approaches in the literature. The aim of this paper is to a) provide a comprehensive review of developed path prediction methods, b) individuate and classify the proposed methods from multiple viewpoints, and c) present a framework for better understanding of various aspects in VRUs path prediction problems.},
author = {Bighashdel, Ariyan and Dubbelman, Gijs},
doi = {10.1109/ITSC.2019.8917053},
file = {:D\:/Documents/USB/Bitacora/papers/paper 1.pdf:pdf},
isbn = {9781538670248},
journal = {2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
number = {783190},
pages = {1039--1046},
title = {{A Survey on Path Prediction Techniques for Vulnerable Road Users: From Traditional to Deep-Learning Approaches}},
year = {2019}
}
